{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Apriori.py\n",
    "#\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "%load_ext memory_profiler\n",
    "\n",
    "class Apriori(object):\n",
    "    def __init__(self, minSupp, minConf):\n",
    "        \"\"\" Parameters setting\n",
    "        \"\"\"\n",
    "        self.minSupp = minSupp  # min support (used for mining frequent sets)\n",
    "        self.minConf = minConf  # min confidence (used for mining association rules)\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\" Run the apriori algorithm, return the frequent *-term sets.\n",
    "        \"\"\"\n",
    "        # Initialize some variables to hold the tmp result\n",
    "        transListSet = data  # 取得transaction list\n",
    "        itemSet = self.getOneItemSet(transListSet)  # get 1-item set\n",
    "        itemCountDict = defaultdict(int)  # key=candiate k-item(k=1/2/...), value=count ,為不存在的 key 設定預設值int\n",
    "        freqSet = dict()  # 儲存所有的 frequent *-items set\n",
    "\n",
    "        self.transLength = len(transListSet)  # number of transactions\n",
    "        self.itemSet = itemSet\n",
    "\n",
    "        # Get the frequent 1-term set\n",
    "        freqOneTermSet = self.getItemsWithMinSupp(transListSet, itemSet, itemCountDict, self.minSupp)  # L1\n",
    "\n",
    "        # Main loop\n",
    "        k = 1\n",
    "        currFreqTermSet = freqOneTermSet  # 當前從L1開始計算\n",
    "        while currFreqTermSet != set():  # 假設當前Lk不為空則迭代進行計算\n",
    "            freqSet[k] = currFreqTermSet  # 儲存Lk items set\n",
    "            k += 1  # k提升一階\n",
    "            currCandiItemSet = self.getJoinedItemSet(currFreqTermSet, k)  # get new candiate k-terms set\n",
    "            # get new frequent k-terms set\n",
    "            currFreqTermSet = self.getItemsWithMinSupp(transListSet, currCandiItemSet,itemCountDict, self.minSupp)\n",
    "\n",
    "        #\n",
    "        self.itemCountDict = itemCountDict  # 儲存所有候選項以及出現的次數(不僅僅是頻繁項),用來計算置信度\n",
    "        self.freqSet = freqSet  # dict: freqSet[k] indicate frequent k-term set Lk)\n",
    "        return itemCountDict, freqSet\n",
    "\n",
    "    def getSpecRules(self, rhs):\n",
    "        \"\"\" Specify a right item, construct rules for it\n",
    "        \"\"\"\n",
    "        if rhs not in self.itemSet: # rhs: L1中的某一個item\n",
    "            print('Please input a term contain in the term-set !')\n",
    "            return None\n",
    "\n",
    "        rules = dict()\n",
    "        for key, value in self.freqSet.items(): #從所有的frequent items set中尋找\n",
    "            for itemSet in value:\n",
    "                if rhs.issubset(itemSet) and len(itemSet) > 1: #假設rhs是屬於frequent itemset 中的子集，且該itemSet長度大於1\n",
    "                    item_supp = self.getSupport(itemSet) #取得該itemSet的support值\n",
    "                    itemSet = itemSet.difference(rhs) #取得該itemSet中的parent itemSet\n",
    "                    conf = item_supp / self.getSupport(itemSet) # cond = sup(itemSet) / sup(parent itemSet)\n",
    "                    if conf >= self.minConf:\n",
    "                        rules[itemSet] = conf # 符合最小confidence值集加入規則\n",
    "        return rules\n",
    "\n",
    "    def getSupport(self, item):\n",
    "        \"\"\" Get the support of item \"\"\"\n",
    "        # return self.itemCountDict[item] / self.transLength\n",
    "        return self.itemCountDict[item]\n",
    "\n",
    "    def getJoinedItemSet(self, termSet, k):\n",
    "        \"\"\" Generate new k-terms candiate itemset\"\"\"\n",
    "        return set([term1.union(term2) for term1 in termSet for term2 in termSet\n",
    "                    if len(term1.union(term2)) == k])\n",
    "        # 取Lk-1中的集合元素，兩兩互配形成Ck\n",
    "\n",
    "    def getOneItemSet(self, transListSet):\n",
    "        itemSet = set()\n",
    "        for line in transListSet:\n",
    "            for item in line:\n",
    "                itemSet.add(frozenset([item]))  # 一一取出並納入集合中\n",
    "        return itemSet\n",
    "\n",
    "    def getItemsWithMinSupp(self, transListSet, itemSet, freqSet, minSupp):\n",
    "        \"\"\" Get frequent item set using min support\n",
    "        \"\"\"\n",
    "        itemSet_ = set()\n",
    "        localSet_ = defaultdict(int)\n",
    "        for item in itemSet:\n",
    "            # 統計items在每筆transaction的出現次數(C1,...Cn)\n",
    "            freqSet[item] += sum([1 for trans in transListSet if item.issubset(trans)])  # 納入frequent itemset集合\n",
    "            localSet_[item] += sum([1 for trans in transListSet if item.issubset(trans)])  # 納入local frequent itemset集合\n",
    "\n",
    "        # Only conserve frequent item-set\n",
    "        n = len(transListSet)  # 取得Transaction Set長度以計算support\n",
    "        for item, cnt in localSet_.items():\n",
    "            # itemSet_.add(item) if float(cnt) / n >= minSupp else None\n",
    "            itemSet_.add(item) if float(cnt) >= minSupp else None  # 統計符合minSupp的frequent items(L1,...Ln)\n",
    "\n",
    "        return itemSet_\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "def loadFromKaggle():\n",
    "\n",
    "    bakery_data = pd.read_csv('BreadBasket_DMS.csv', encoding='utf-8')\n",
    "    bakery_data['Date Time'] = bakery_data['Date'] + \" \" + bakery_data['Time']\n",
    "    bakery_data = bakery_data.drop(['Date', 'Time'], axis=1)\n",
    "    bakery_data = bakery_data.drop(['Date Time'], axis=1)\n",
    "    bakery_data = bakery_data[~bakery_data['Item'].str.contains('NONE')]\n",
    "\n",
    "    tdl = []\n",
    "    for i in range(1, bakery_data.Transaction.count() + 1):\n",
    "        tdf = bakery_data[bakery_data.Transaction == i]\n",
    "        l = set()\n",
    "        for j in range(0, tdf.Transaction.count()):\n",
    "            l.add(tdf.Item.iloc[j])\n",
    "        if len(l) > 0:\n",
    "            tdl.append(list(l))\n",
    "        else:\n",
    "            tdl.append(None)\n",
    "\n",
    "    col = ['items']\n",
    "    TDB = pd.DataFrame({\"items\": tdl}, columns=col)\n",
    "    TDB = TDB.dropna()\n",
    "    return TDB['items'].tolist()\n",
    "\n",
    "\n",
    "def loadFromIbm():\n",
    "    # read data from Ibm\n",
    "    with open('data', encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    Transaction = []  # to store transaction\n",
    "    Frequent_items_value = {}  # to store all frequent item sets\n",
    "\n",
    "    # to fill values in transaction from txt file\n",
    "    Transaction = {}\n",
    "    Transaction\n",
    "    for i in range(0, len(content)):\n",
    "        rowd = content[i].split(' ')\n",
    "        rowd = [r for r in rowd if r != '']\n",
    "        rowd = rowd[1:]\n",
    "        if Transaction.get(rowd[0], None) == None:\n",
    "            Transaction[rowd[0]] = [rowd[1]]\n",
    "        else:\n",
    "            Transaction[rowd[0]].append(rowd[1])\n",
    "    # print(type(list(Transaction.values())))\n",
    "    return list(Transaction.values())\n",
    "\n",
    "\n",
    "# cd D:\\WorkSpace\\PythonWorkSpace\\Apriori\n",
    "# D:\n",
    "# python Apriori.py ibm 2 0.6\n",
    "def test1():\n",
    "    fn = 'BreadBasket_DMS.csv'  # BreadBasket_DMS.csv\n",
    "    minSup = 16  # 3\n",
    "    minConf = 0.6  # 0.6\n",
    "    print('fileName = {} ,minSup= {} , minConf={}'.format(fn, minSup, minConf))\n",
    "\n",
    "    starttime = time.time()\n",
    "#     if fn == 'kaggle':\n",
    "#     dataSet = loadFromKaggle()\n",
    "#     elif fn == 'ibm':\n",
    "#         dataSet = loadFromIbm()\n",
    "    dataSet = loadFromIbm()\n",
    "    # Run\n",
    "    objApriori = Apriori(minSup, minConf)\n",
    "    itemCountDict, freqSet = objApriori.fit(dataSet)\n",
    "    #     print(itemCountDict)\n",
    "    endtime = time.time()\n",
    "    print(\"\\nTime Taken is: {0:.2f}ms \\n\".format((endtime - starttime)))\n",
    "    #   print\n",
    "    for key, value in freqSet.items():\n",
    "        print('{}-Itemsets:{}'.format(key, len(value)))\n",
    "        print('-' * 20)\n",
    "        for itemset in value:\n",
    "            print(\"Items :{} , Support:{} \".format(list(itemset), itemCountDict[itemset]))\n",
    "        print()\n",
    "\n",
    "    # Return rules with regard of `rhs`\n",
    "    print()\n",
    "    print('List All Rules:')\n",
    "    print()\n",
    "    L1 = set(freqSet[1])\n",
    "    for rhs in L1:\n",
    "        rules = objApriori.getSpecRules(rhs)\n",
    "        if len(rules) > 0:\n",
    "            #             print('-'*20)\n",
    "            #             print('rules refer to {}'.format(list(rhs)))\n",
    "            for key, value in rules.items():\n",
    "                print('Rule : {} -> {}, confidence = {}'.format(list(key), list(rhs), value))\n",
    "#     %memit test()\n",
    "# %mprun -T mprof0 -f test test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 87.71 MiB, increment: 0.06 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "# function to get frequent one itemset\n",
    "def frequent_one_item(Transaction, min_support):\n",
    "\tcandidate1 = {}\n",
    "\n",
    "\tfor i in range(0, len(Transaction)):  # 計數C1 item中的出現次數\n",
    "\t\tfor j in range(0, len(Transaction[i])):\n",
    "\t\t\tif Transaction[i][j] not in candidate1:\n",
    "\t\t\t\tcandidate1[Transaction[i][j]] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tcandidate1[Transaction[i][j]] += 1\n",
    "\n",
    "\tfrequentitem1 = []  # to get frequent 1 itemsets with minimum support count\n",
    "\tfor value in candidate1:\n",
    "\t\tif candidate1[value] >= min_support:\n",
    "\t\t\tfrequentitem1 = frequentitem1 + [[value]]\n",
    "\t\t\tFrequent_items_value[tuple(value)] = candidate1[value]\n",
    "\n",
    "\treturn frequentitem1\n",
    "\n",
    "\n",
    "def printL1(L1):\n",
    "\tprint(\"1-itemsets :{}\".format(len(L1.items())))\n",
    "\tfor key, value in L1.items():\n",
    "\t\tkey = str(key).replace('(', '[').replace(')', ']')\n",
    "\t\tprint(\"Items: {}, Support :{}\".format(key, value))\n",
    "\n",
    "\n",
    "def print_L(L_value):\n",
    "\tfor i, L in enumerate(L_value):\n",
    "\t\tif i < 2:\n",
    "\t\t\tpass\n",
    "\t\telif len(L) == 0:\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tprint()\n",
    "\t\t\tprint(\"{}-itemsets :{}\".format((i), len(L)))\n",
    "\t\t\t[print('Items: {} , Support: '.format(itemSet)) for itemSet in L]\n",
    "\n",
    "\n",
    "# class of Hash node\n",
    "class Hash_node:\n",
    "\tdef __init__(self):\n",
    "\t\tself.children = {}  # pointer to its children\n",
    "\t\tself.Is_Leaf = True  # to know the status whether current node is leaf or not\n",
    "\t\tself.bucket = {}  # contains itemsets in bucket\n",
    "\n",
    "\n",
    "# class of constructing and getting hashtree\n",
    "class HashTree:\n",
    "\t# class constructor\n",
    "\tdef __init__(self, max_leaf_count, max_child_count):\n",
    "\t\tself.root = Hash_node()\n",
    "\t\tself.max_leaf_count = max_leaf_count\n",
    "\t\tself.max_child_count = max_child_count\n",
    "\t\tself.frequent_itemsets = []\n",
    "\n",
    "\t# function to recursive insertion to make hashtree\n",
    "\tdef recursively_insert(self, node, itemset, index, count):\n",
    "\t\tif index == len(itemset):  # 如果當前插入索引已滿\n",
    "\t\t\tif itemset in node.bucket:  # bucket : HashTree的緩衝儲存區\n",
    "\t\t\t\tnode.bucket[itemset] += count  # 若ck itemset已存在bucket中，則在bucket中計數值加一次count\n",
    "\t\t\telse:\n",
    "\t\t\t\tnode.bucket[itemset] = count  # 若ck itemset已存在bucket中，則在bucket中計數值等於當前count\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif node.Is_Leaf:  # if node is leaf\n",
    "\t\t\tif itemset in node.bucket:\n",
    "\t\t\t\tnode.bucket[itemset] += count  # 若ck itemset已存在bucket中，則在bucket中計數值加一次count\n",
    "\t\t\telse:\n",
    "\t\t\t\tnode.bucket[itemset] = count  # 若ck itemset已存在bucket中，則在bucket中計數值等於當前count\n",
    "\t\t\tif len(node.bucket) == self.max_leaf_count:  # 如果bucket可容納的數量已經達到達到leaf node的乘載最大數量\n",
    "\t\t\t\tfor old_itemset, old_count in node.bucket.items():  # 取出bucket中舊有的itemset和count\n",
    "\n",
    "\t\t\t\t\thash_key = self.hash_function(old_itemset[index])  # 將其 hashing 至另外一個 index\n",
    "\t\t\t\t\tif hash_key not in node.children:  # 如果Node底下並沒有以hash_key為首的children HT\n",
    "\t\t\t\t\t\tnode.children[hash_key] = Hash_node()  # 在Node底下做一棵以hash_key為首的children HT\n",
    "\t\t\t\t\tself.recursively_insert(node.children[hash_key], old_itemset, index + 1, old_count)\n",
    "\t\t\t\t# 將bucket中舊有的itemset遞迴插入至children HT\n",
    "\t\t\t\t# since no more requirement of this bucket\n",
    "\t\t\t\tdel node.bucket  # 將緩衝儲存區的內容清除\n",
    "\t\t\t\tnode.Is_Leaf = False  # 該節點成為Non-Leaf node\n",
    "\t\telse:  # if node is not leaf\n",
    "\t\t\thash_key = self.hash_function(itemset[index])\n",
    "\t\t\tif hash_key not in node.children:  # 如果Non-Leaf node底下並沒有以hash_key為首的children HT\n",
    "\t\t\t\tnode.children[hash_key] = Hash_node()  # 在Node底下做一棵以hash_key為首的children HT\n",
    "\t\t\tself.recursively_insert(node.children[hash_key], itemset, index + 1, count)\n",
    "\t\t# 將itemset遞迴插入至指定的Hash children HT\n",
    "\n",
    "\tdef insert(self, itemset):\n",
    "\t\titemset = tuple(itemset)  # 將ck中的itemset轉化成dict結構\n",
    "\t\tself.recursively_insert(self.root, itemset, 0, 0)  # 進行遞迴插入\n",
    "\n",
    "\t# to add support to candidate itemsets. Transverse the Tree and find the bucket in which this itemset is present.\n",
    "\tdef add_support(self, itemset):\n",
    "\t\tTransverse_HNode = self.root\n",
    "\t\titemset = tuple(itemset)\n",
    "\t\tindex = 0\n",
    "\t\twhile True:\n",
    "\t\t\tif Transverse_HNode.Is_Leaf:\n",
    "\t\t\t\tif itemset in Transverse_HNode.bucket:  # found the itemset in this bucket\n",
    "\t\t\t\t\tTransverse_HNode.bucket[itemset] += 1  # increment the count of this itemset.\n",
    "\t\t\t\tbreak\n",
    "\t\t\thash_key = self.hash_function(itemset[index])\n",
    "\t\t\tif hash_key in Transverse_HNode.children:\n",
    "\t\t\t\tTransverse_HNode = Transverse_HNode.children[hash_key]\n",
    "\t\t\telse:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tindex += 1\n",
    "\n",
    "\t# to transverse the hashtree to get frequent itemsets with minimum support count\n",
    "\tdef get_frequent_itemsets(self, node, support_count, frequent_itemsets):\n",
    "\t\tif node.Is_Leaf:\n",
    "\t\t\tfor key, value in node.bucket.items():\n",
    "\t\t\t\tif value >= support_count:  # if it satisfies the condition\n",
    "\t\t\t\t\tfrequent_itemsets.append(list(key))  # then add it to frequent itemsets.\n",
    "\t\t\t\t\tFrequent_items_value[key] = value\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tfor child in node.children.values():\n",
    "\t\t\tself.get_frequent_itemsets(child, support_count, frequent_itemsets)\n",
    "\n",
    "\t# hash function for making HashTree\n",
    "\tdef hash_function(self, val):\n",
    "\t\treturn int(val) % self.max_child_count\n",
    "\n",
    "\n",
    "# To generate hash tree from candidate itemsets\n",
    "def generate_hash_tree(candidate_itemsets, max_leaf_count, max_child_count):\n",
    "\thtree = HashTree(max_child_count, max_leaf_count)  # create instance of HashTree\n",
    "\tfor itemset in candidate_itemsets:\n",
    "\t\thtree.insert(itemset)  # to insert itemset into Hashtree\n",
    "\treturn htree\n",
    "\n",
    "\n",
    "# to generate subsets of itemsets of size k\n",
    "def generate_k_subsets(dataset, length):\n",
    "\tsubsets = []\n",
    "\tfor itemset in dataset:\n",
    "\t\tsubsets.extend(map(list, itertools.combinations(itemset, length)))\n",
    "\treturn subsets\n",
    "\n",
    "\n",
    "def subset_generation(ck_data, l):\n",
    "\t# itertools.combinations(iterable, r)\n",
    "\t# 創建一個迭代器，返回iterable中所有長度為r的子序列，返回的子序列中的項按輸入iterable中的順序排序（不帶重複）\n",
    "\treturn map(list, set(itertools.combinations(ck_data, l)))\n",
    "\n",
    "\n",
    "# apriori generate function to generate ck\n",
    "def apriori_generate(Lk, k):\n",
    "\tck = []\n",
    "\t# join step\n",
    "\tlenlk = len(Lk)\n",
    "\tfor i in range(lenlk):  # 從Lk中的itemsets取出元素相互混種\n",
    "\t\tfor j in range(i + 1, lenlk):\n",
    "\t\t\tL1 = list(Lk[i])[:k - 2]\n",
    "\t\t\tL2 = list(Lk[j])[:k - 2]\n",
    "\t\t\tif L1 == L2:\n",
    "\t\t\t\tck.append(sorted(list(set(Lk[i]) | set(Lk[j]))))  # 將新混種出來的itemsets放入Ck\n",
    "\n",
    "\t\t\t# prune step\n",
    "\tfinal_ck = []\n",
    "\tfor candidate in ck:\n",
    "\t\tall_subsets = list(subset_generation(set(candidate), k - 1))  # 產生Ck-1的Subsets\n",
    "\t\tfound = True\n",
    "\n",
    "\t\t# -- 改良--#\n",
    "\t\tfor subset in all_subsets:\n",
    "\t\t\tsubset = list(sorted(subset))\n",
    "\t\t\tif (subset not in Lk) and (subset in ck):\n",
    "\t\t\t\tck.remove(subset)\n",
    "\n",
    "\t\t\t# -- 舊有--#\n",
    "\t\t\t# for i in range(len(all_subsets)):\n",
    "\t\t\t#     value = list(sorted(all_subsets[i]))\n",
    "\t\t\t#     if value not in Lk:\n",
    "\t\t\t#         found = False\n",
    "\t\t\t# if found == True:\n",
    "\t\t\t#     final_ck.append(candidate)\n",
    "\n",
    "\treturn ck, final_ck\n",
    "\n",
    "\n",
    "def generateL(ck, min_support):\n",
    "\tsupport_ck = {}\n",
    "\tfor val in Transaction1:\n",
    "\t\tfor val1 in ck:\n",
    "\t\t\tvalue = set(val)\n",
    "\t\t\tvalue1 = set(val1)\n",
    "\n",
    "\t\t\tif value1.issubset(value):\n",
    "\t\t\t\tif tuple(val1) not in support_ck:\n",
    "\t\t\t\t\tsupport_ck[tuple(val1)] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsupport_ck[tuple(val1)] += 1\n",
    "\tfrequent_item = []\n",
    "\tfor item_set in support_ck:\n",
    "\t\tif support_ck[item_set] >= min_support:\n",
    "\t\t\tfrequent_item.append(sorted(list(item_set)))\n",
    "\t\t\tFrequent_items_value[item_set] = support_ck[item_set]\n",
    "\n",
    "\treturn frequent_item\n",
    "\n",
    "\n",
    "# main apriori algorithm function\n",
    "def apriori(L1, min_support, max_leaf_count=3, max_child_count=5):\n",
    "\tk = 2;\n",
    "\tL = []\n",
    "\tL.append(0)\n",
    "\tL.append(L1)\n",
    "\n",
    "\tstart = time.time()\n",
    "\twhile (len(L[k - 1]) > 0):  # 假如Lk-1裡面還有itemsets就繼續做\n",
    "\t\tck, final_ck = apriori_generate(L[k - 1], k)  # to generate candidate itemsets\n",
    "\t\th_tree = generate_hash_tree(ck, max_leaf_count, max_child_count)  # to generate hashtree\n",
    "\t\tk_subsets = generate_k_subsets(Transaction1, k)  # to generate subsets of each transaction\n",
    "\t\tfor subset in k_subsets:\n",
    "\t\t\th_tree.add_support(subset)  # hashtree中的每個node加上support\n",
    "\t\tlk = []\n",
    "\t\th_tree.get_frequent_itemsets(h_tree.root, min_support, lk)  # 從hashtree中取出每個 frequent itemsets\n",
    "\t\tL.append(lk)\n",
    "\t\tk = k + 1  # Ck,Lk提升一階\n",
    "\tend = time.time()\n",
    "\treturn L, (end - start)\n",
    "\n",
    "\n",
    "def generateL1(L1, Transaction_len):\n",
    "\tprint(\"1-itemsets :{}\".format(len(L1.items())))\n",
    "\tcpL1 = copy.deepcopy(L1)\n",
    "\n",
    "\tfor key, value in cpL1.items():\n",
    "\t\t# key = str(key).replace('(', '[').replace(')', ']')\n",
    "\t\t# key = int(key)\n",
    "\t\titem = 0\n",
    "\t\tfor i in range(0,len(key)):\n",
    "\t\t\titem = item + int(key[i])\n",
    "\t\t\titem*=10\n",
    "\t\t\tL1[item] = value\n",
    "\t\tdel L1[key]\n",
    "\t\tprint(\"Items: {} , Support :{}\".format([item], value))\n",
    "\n",
    "\n",
    "def generateLn(L_value):\n",
    "\tfor i, L in enumerate(L_value):\n",
    "\t\tif i < 2:\n",
    "\t\t\tpass\n",
    "\t\telif len(L) == 0:\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tprint()\n",
    "\t\t\tprint(\"{}-itemsets :{}\".format((i), len(L)))\n",
    "\t\t\t[print('Items: {} , Support:{}'.format(itemSet,Frequent_items_value[tuple(itemSet)])) for itemSet in L]\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\timport itertools\n",
    "\timport time\n",
    "\n",
    "\n",
    "def loadFromKaggle():\n",
    "\n",
    "    bakery_data = pd.read_csv('BreadBasket_DMS.csv', encoding='utf-8')\n",
    "    bakery_data['Date Time'] = bakery_data['Date'] + \" \" + bakery_data['Time']\n",
    "    bakery_data = bakery_data.drop(['Date', 'Time'], axis=1)\n",
    "    bakery_data = bakery_data.drop(['Date Time'], axis=1)\n",
    "    bakery_data = bakery_data[~bakery_data['Item'].str.contains('NONE')]\n",
    "\n",
    "    tdl = []\n",
    "    for i in range(1, bakery_data.Transaction.count() + 1):\n",
    "        tdf = bakery_data[bakery_data.Transaction == i]\n",
    "        l = set()\n",
    "        for j in range(0, tdf.Transaction.count()):\n",
    "            l.add(tdf.Item.iloc[j])\n",
    "        if len(l) > 0:\n",
    "            tdl.append(list(l))\n",
    "        else:\n",
    "            tdl.append(None)\n",
    "\n",
    "    col = ['items']\n",
    "    TDB = pd.DataFrame({\"items\": tdl}, columns=col)\n",
    "    TDB = TDB.dropna()\n",
    "    return TDB['items'].tolist()\n",
    "def loadFromIbm():\n",
    "    # read data from Ibm\n",
    "    with open('data', encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    Transaction = []  # to store transaction\n",
    "    Frequent_items_value = {}  # to store all frequent item sets\n",
    "\n",
    "    # to fill values in transaction from txt file\n",
    "    Transaction = {}\n",
    "    Transaction\n",
    "    for i in range(0, len(content)):\n",
    "        rowd = content[i].split(' ')\n",
    "        rowd = [r for r in rowd if r != '']\n",
    "        rowd = rowd[1:]\n",
    "        if Transaction.get(rowd[0], None) == None:\n",
    "            Transaction[rowd[0]] = [rowd[1]]\n",
    "        else:\n",
    "            Transaction[rowd[0]].append(rowd[1])\n",
    "    # print(type(list(Transaction.values())))\n",
    "    return list(Transaction.values())\n",
    "\n",
    "\n",
    "\n",
    "def test2():\n",
    "    # cd D:\\WorkSpace\\PythonWorkSpace\\Apriori\n",
    "    # D:\n",
    "    # python Apriori_HT.py ibm 10 3 5\n",
    "#     fn = str(sys.argv[1])\n",
    "    minSup = 16  # 3\n",
    "    max_leaf_count = 3  # 3\n",
    "    max_child_count = 5  # 5\n",
    "    print('fileName = {} ,minSup= {} , max_leaf_count={} , max_child_count={} '.format(fn, minSup, max_leaf_count,max_child_count))\n",
    "\n",
    "    starttime = time.time()\n",
    "\n",
    "    Frequent_items_value = {}  # to store all frequent item sets\n",
    "\n",
    "#     if fn == 'ibm':\n",
    "    Transaction = loadFromIbm()\n",
    "\n",
    "    Transaction_len = len(Transaction)\n",
    "\n",
    "    print(\"All frequent itemsets with their support count:\")\n",
    "    L1 = frequent_one_item(Transaction, minSup)\n",
    "    generateL1(Frequent_items_value, Transaction_len)\n",
    "\n",
    "    # to remove infrequent 1 itemsets from transaction\n",
    "    Transaction1 = []\n",
    "    for i in range(0, len(Transaction)):\n",
    "        list_val = []\n",
    "        for j in range(0, len(Transaction[i])):\n",
    "            if [Transaction[i][j]] in L1:\n",
    "                list_val.append(Transaction[i][j])\n",
    "        Transaction1.append(list_val)\n",
    "\n",
    "    L_value, time_taken = apriori(L1, minSup, max_leaf_count, max_child_count)\n",
    "    generateLn(L_value)\n",
    "\n",
    "    print(\"\\nTime Taken is: {0:.3f} ms\\n\".format(time_taken))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 89.52 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "# %memit test1\n",
    "%memit test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer2FrozenDataSet(dataSet):\n",
    "    frozenDataSet = {}\n",
    "    for elem in dataSet:\n",
    "        frozenDataSet[frozenset(elem)] = 1\n",
    "    return frozenDataSet\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, nodeName, count, nodeParent):\n",
    "        self.nodeName = nodeName\n",
    "        self.count = count\n",
    "        self.nodeParent = nodeParent\n",
    "        self.nextSimilarItem = None  # 指向下一個相同元素的指針nextSimilarItem\n",
    "        self.children = {}\n",
    "\n",
    "    def updateC(self, count):\n",
    "        self.count += count\n",
    "\n",
    "\n",
    "def createFPTree(frozenDataSet, minSupport):\n",
    "    # scan dataset at the first time, filter out items which are less than minSupport\n",
    "    frequenctNodeTable = {}  # 紀錄每個item的出現數目,還有其parent Node\n",
    "    for transactions in frozenDataSet:  # 依序取出每筆transactions\n",
    "        for item in transactions:  # 在取出每項item\n",
    "            item = str(item)\n",
    "            frequenctNodeTable[item] = frequenctNodeTable.get(item, 0) + frozenDataSet[transactions]  # 取出每項的count+1\n",
    "\n",
    "\n",
    "            # count = frequenctNodeTable.get(item, default = 0) , 1 = frozenDataSet[transactions]\n",
    "    frequenctNodeTable = {k: v for k, v in frequenctNodeTable.items() if v >= minSupport}  # 濾除掉不滿足minSupport的item\n",
    "    frequentItemSet = set(frequenctNodeTable.keys())  # 紀錄滿足minSupport items的集合\n",
    "\n",
    "    if len(frequentItemSet) == 0: return None, None\n",
    "\n",
    "    for k in frequenctNodeTable:\n",
    "        # 將frequenctNodeTable中的結構(item,count) -> (item,[count,ItemNode(default=None)])\n",
    "        frequenctNodeTable[k] = [frequenctNodeTable[k], None]  # (初始化所有非root Node的所有資訊)\n",
    "\n",
    "    fptree = TreeNode(\"null\", 1, None)  # 初始化FP-Tree Root Node\n",
    "    # scan dataset at the second time, filter out items for each record\n",
    "    for transactions, count in frozenDataSet.items():  # 再次scanfrozenDataSet中的所有items\n",
    "        frequentItemsInRecord = {}  # 紀錄transaction中的frequent items\n",
    "        for item in transactions:\n",
    "            if item in frequentItemSet:  # 利用frequentItemSet過濾出每筆transaction中的常見items\n",
    "                frequentItemsInRecord[item] = frequenctNodeTable[item][0]\n",
    "        if len(frequentItemsInRecord) > 0:\n",
    "            # 將transaction中的items依照support大小作排序(大的在前)，形成一條FP path\n",
    "            orderedFrequentItems = [v[0] for v in sorted(frequentItemsInRecord.items(), key=lambda v: v[1],\n",
    "                                                         reverse=True)]  # sort by v[1] (support)降序\n",
    "            updateFPTree(fptree, orderedFrequentItems, frequenctNodeTable, count)\n",
    "\n",
    "    return fptree, frequenctNodeTable\n",
    "\n",
    "\n",
    "def updateFPTree(fptree, orderedFrequentItems, frequenctNodeTable, count):\n",
    "    # handle the first item\n",
    "    firstOrderItem = orderedFrequentItems[0]  # scan FP-path 的第1個item\n",
    "    if firstOrderItem in fptree.children:  # 如果firstOrderItem是fptree 節點的子節點\n",
    "        fptree.children[firstOrderItem].updateC(count)  # 增加該子節點的count數\n",
    "    else:\n",
    "        fptree.children[firstOrderItem] = TreeNode(firstOrderItem, count,\n",
    "                                                   fptree)  # 否則，需要新創建一個TreeNode 節點，然後將其賦給fptree 節點的子節點\n",
    "        # update frequenctNodeTable\n",
    "        if frequenctNodeTable[firstOrderItem][1] == None:  # 如果frequenctNodeTable裡記錄的firstOrderItem Node為None\n",
    "            frequenctNodeTable[firstOrderItem][1] = fptree.children[\n",
    "                firstOrderItem]  # 則更新frequenctNodeTable裡的firstOrderItem Node為自己本身\n",
    "        else:\n",
    "            updateFrequenctNodeTable(frequenctNodeTable[firstOrderItem][1],\n",
    "                                     fptree.children[firstOrderItem])  # 更新 firstOrderItem 下一個相似元素指標\n",
    "    # handle other items except the first item\n",
    "    if (len(orderedFrequentItems) > 1):  # 假如 FP-path 的後面還有其他item\n",
    "        updateFPTree(fptree.children[firstOrderItem], orderedFrequentItems[1:], frequenctNodeTable, count)\n",
    "        # 以FP-path上的firstOrderItem node為新的root Node，處理後續的orderedFrequentItems\n",
    "\n",
    "\n",
    "def updateFrequenctNodeTable(headFrequentNode, targetNode):\n",
    "    while (headFrequentNode.nextSimilarItem != None):\n",
    "        headFrequentNode = headFrequentNode.nextSimilarItem  # 不斷尋找相似元素指標為空的headFrequentNode\n",
    "    headFrequentNode.nextSimilarItem = targetNode  # 將headFrequentNode 與 targetNode 以相似元素指標連接\n",
    "\n",
    "\n",
    "def mining_FPTree(frequenctNodeTable, suffix, frequentPatterns, minSupport):\n",
    "    # for each item in frequenctNodeTable, find conditional suffix path, create conditional fptree, then iterate until there is only one element in conditional fptree\n",
    "    if (frequenctNodeTable == None): return\n",
    "    frequenctItems = [v[0] for v in sorted(frequenctNodeTable.items(),\n",
    "                                           key=lambda v: v[1][0])]  # 將frequenctNodeTable以升序排序，從low frequent開始mining\n",
    "    if (len(frequenctItems) == 0): return\n",
    "\n",
    "    for frequenctItem in frequenctItems:\n",
    "        newSuffix = suffix.copy()  # 只拷貝父級目錄，子目錄不拷貝\n",
    "        newSuffix.add(frequenctItem)  # 加入一個擁有最低出現次數的Suffix item作為newSuffix\n",
    "        support = frequenctNodeTable[frequenctItem][0]  # 逐一取得Suffix item的support\n",
    "        frequentPatterns[frozenset(newSuffix)] = support  # 逐一取得Suffix item的support，並將其一一加入到frequentPatterns\n",
    "\n",
    "        suffixPath = getSuffixPath(frequenctNodeTable, frequenctItem)  # Find suffix patterns to the frequenctItem\n",
    "        if (suffixPath != {}):\n",
    "            conditionalFPtree, conditionalFNodeTable = createFPTree(suffixPath, minSupport)\n",
    "            # 根據suffixPath建立conditionalFPtree 和 conditional frequenctNode Table\n",
    "            if conditionalFNodeTable != None:  # 假如 conditional frequenctNode不為空\n",
    "                mining_FPTree(conditionalFNodeTable, newSuffix, frequentPatterns,\n",
    "                              minSupport)  # 繼續挖掘剩餘的conditionalFPtree\n",
    "\n",
    "\n",
    "def getSuffixPath(frequenctNodeTable, frequenctItem):\n",
    "    suffixPath = {}\n",
    "    beginNode = frequenctNodeTable[frequenctItem][1]  # 指向當前frequenctItem 的 Tree Node\n",
    "    suffixs = ascendNodeList(beginNode)  # suffixs紀錄由beginNode往上層尋找的ascendTree (不包含beginNode)\n",
    "    if ((suffixs != [])):\n",
    "        suffixPath[\n",
    "            frozenset(suffixs)] = beginNode.count  # 建立一條由suffix Node起始但不包含suffix Node的完整suffixPath，數量為beginNode的出現次數\n",
    "\n",
    "    while (beginNode.nextSimilarItem != None):\n",
    "        beginNode = beginNode.nextSimilarItem\n",
    "        suffixs = ascendNodeList(beginNode)\n",
    "        if (suffixs != []):\n",
    "            suffixPath[frozenset(suffixs)] = beginNode.count\n",
    "\n",
    "    return suffixPath  # 回傳suffixPath\n",
    "\n",
    "\n",
    "def ascendNodeList(treeNode):\n",
    "    suffixs = []\n",
    "    while ((treeNode.nodeParent != None) and (treeNode.nodeParent.nodeName != 'null')):\n",
    "        treeNode = treeNode.nodeParent\n",
    "        suffixs.append(treeNode.nodeName)\n",
    "    return suffixs\n",
    "\n",
    "\n",
    "def rulesGenerator(frequentPatterns, minConf, rules):\n",
    "    for frequentset in frequentPatterns:\n",
    "        if (len(frequentset) > 1):  # 依序取得所有長度大於1的frequentset\n",
    "            getRules(frequentset, frequentset, rules, frequentPatterns, minConf)\n",
    "            # 透過frequentset自己本身去產生規則並輸出到rules內\n",
    "\n",
    "\n",
    "def removeStr(set, str):\n",
    "    tempSet = []\n",
    "    for elem in set:\n",
    "        if (elem != str):\n",
    "            tempSet.append(elem)  # 將不等於frequentElem的項目篩選出來，並加入到tempSet中\n",
    "    tempFrozenSet = frozenset(tempSet)\n",
    "    return tempFrozenSet\n",
    "\n",
    "\n",
    "def getRules(frequentset, currentset, rules, frequentPatterns, minConf):  # 由大至小拆解各集合，以取得規則\n",
    "    for frequentElem in currentset:\n",
    "        subSet = removeStr(currentset, frequentElem)  # subSet為不包含frequentElem的子集合\n",
    "        # print(currentset)\n",
    "        # print(frequentElem)\n",
    "        confidence = frequentPatterns.get(frequentset) / frequentPatterns.get(subSet, 9999)\n",
    "        # confidence = sup(currentset) / sup(subSet)\n",
    "        if (confidence >= minConf):\n",
    "            flag = False\n",
    "            for rule in rules:\n",
    "                # rule[0] : 推演規則  # rule[1] : 衍伸規則\n",
    "                if (rule[0] == subSet and rule[1] == frequentset - subSet):\n",
    "                    flag = True\n",
    "            if (flag == False):\n",
    "                rules.append((subSet, frequentset - subSet, confidence))\n",
    "                # rules.append (推演規則 -------> 衍伸規則 , confidence)\n",
    "\n",
    "            if (len(subSet) >= 2):  # 如果subSet中的frequent items數大於2\n",
    "                getRules(frequentset, subSet, rules, frequentPatterns, minConf)\n",
    "                # 以subSet為目標繼續挖掘其他的規則\n",
    "\n",
    "\n",
    "def getpatterns(pattern):\n",
    "    maxVal = max(pattern.items())[1]\n",
    "    dictpattern = {}\n",
    "    for i in range(1, maxVal + 10):\n",
    "        dictpattern[str(i)] = []\n",
    "\n",
    "    for key, value in pattern.items():\n",
    "        dictpattern[str(len(key))].append([key, value])\n",
    "\n",
    "    for i in range(1, maxVal + 10):\n",
    "        patterns = dictpattern[str(i)]\n",
    "        if len(patterns) > 0:\n",
    "            print(\"{}-items set :{}\".format(str(i),len(patterns)))\n",
    "            [print(\"items:{} , support:{} \".format(set(item[0]), item[1])) for item in patterns]\n",
    "            print()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "def loadFromKaggle():\n",
    "\n",
    "    bakery_data = pd.read_csv('BreadBasket_DMS.csv', encoding='utf-8')\n",
    "    bakery_data['Date Time'] = bakery_data['Date'] + \" \" + bakery_data['Time']\n",
    "    bakery_data = bakery_data.drop(['Date', 'Time'], axis=1)\n",
    "    bakery_data = bakery_data.drop(['Date Time'], axis=1)\n",
    "    bakery_data = bakery_data[~bakery_data['Item'].str.contains('NONE')]\n",
    "\n",
    "    tdl = []\n",
    "    for i in range(1, bakery_data.Transaction.count() + 1):\n",
    "        tdf = bakery_data[bakery_data.Transaction == i]\n",
    "        l = set()\n",
    "        for j in range(0, tdf.Transaction.count()):\n",
    "            l.add(tdf.Item.iloc[j])\n",
    "        if len(l) > 0:\n",
    "            tdl.append(list(l))\n",
    "        else:\n",
    "            tdl.append(None)\n",
    "\n",
    "    col = ['items']\n",
    "    TDB = pd.DataFrame({\"items\": tdl}, columns=col)\n",
    "    TDB = TDB.dropna()\n",
    "    return TDB['items'].tolist()\n",
    "\n",
    "\n",
    "def loadFromIbm():\n",
    "    # read data from Ibm\n",
    "    with open('data', encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    Transaction = []  # to store transaction\n",
    "    Frequent_items_value = {}  # to store all frequent item sets\n",
    "\n",
    "    # to fill values in transaction from txt file\n",
    "    Transaction = {}\n",
    "    Transaction\n",
    "    for i in range(0, len(content)):\n",
    "        rowd = content[i].split(' ')\n",
    "        rowd = [r for r in rowd if r != '']\n",
    "        rowd = rowd[1:]\n",
    "        if Transaction.get(rowd[0], None) == None:\n",
    "            Transaction[rowd[0]] = [rowd[1]]\n",
    "        else:\n",
    "            Transaction[rowd[0]].append(rowd[1])\n",
    "    # print(type(list(Transaction.values())))\n",
    "    return list(Transaction.values())\n",
    "\n",
    "def test3():\n",
    "    # cd D:\\WorkSpace\\PythonWorkSpace\\Apriori\n",
    "    # D:\n",
    "    # python FP-Growth.py ibm 10 0.6\n",
    "#     fn = str(sys.argv[1])  # BreadBasket_DMS.csv\n",
    "    minSup = 30  # 3\n",
    "    minConf = 0.6  # 0.6\n",
    "    print('fileName = {} ,minSup= {} , minConf={}'.format(fn, minSup, minConf))\n",
    "\n",
    "    starttime = time.time()\n",
    "    # if fn == 'kaggle':\n",
    "    dataSet = loadFromKaggle()\n",
    "    # elif fn == 'ibm':\n",
    "#     dataSet = loadFromIbm()\n",
    "\n",
    "    frozenDataSet = transfer2FrozenDataSet(dataSet)  # 為transaction中所有曾經出現的item建立Node，並初始化support為1\n",
    "    minSupport = 3\n",
    "    fptree, frequenctNodeTable = createFPTree(frozenDataSet, minSup)\n",
    "    frequentPatterns = {}\n",
    "\n",
    "    suffix = set([])\n",
    "    mining_FPTree(frequenctNodeTable, suffix, frequentPatterns, minSup)\n",
    "\n",
    "    rules = []\n",
    "    endtime = time.time()\n",
    "    print(\"fptree:\")\n",
    "    print(\"\\nTime Taken is: {0:.2f}ms \\n\".format((endtime - starttime)))\n",
    "\n",
    "    print(\"frequent patterns:\")\n",
    "    getpatterns(frequentPatterns)\n",
    "\n",
    "    print(\"association rules:\")\n",
    "    rulesGenerator(frequentPatterns, minConf, rules)\n",
    "    rules = [rule for rule in rules if rule != None]\n",
    "    [print('Rules:{}-->{}, confidence:{}'.format(set(r[0]), set(r[1]), r[2])) for r in rules]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 89.35 MiB, increment: 0.02 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit test3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
